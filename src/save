'''
test min_ra, max_ra, min_decl, max_decl on test.csv file
'''
def test_getCoordinatesMinMax(spark_context):
    result=TP2_spark.getCoordinatesMinMax("./Source/test.csv", spark_context)
    assert  result== [0.0, 10.0, 0.0, 10.0]

'''
test nbRows, nbCols on test.csv file
'''
def test_rowsColsOf_MapOfBlocks(spark_context):
    mapOfBlocks = MapOfBlocks.MapOfBlocks(80, 20)
    mapOfBlocks.setCoordinates([0.0, 10.0, 0.0, 10.0])

    assert mapOfBlocks.nbRows == 2
    assert mapOfBlocks.nbCols == 2

'''
test steps on test.csv file
'''
def test_steps_MapOfBlocks(spark_context):
    mapOfBlocks = MapOfBlocks.MapOfBlocks(80, 20)
    mapOfBlocks.setCoordinates([0.0, 10.0, 0.0, 10.0])

    assert mapOfBlocks.step_ra == 5.0
    assert mapOfBlocks.step_decl == 5.0

'''
test number of blocks on test.csv file
'''
def test_nbBlocks_MapOfBlocks(spark_context):
    mapOfBlocks = MapOfBlocks.MapOfBlocks(80, 20)
    mapOfBlocks.setCoordinates([0.0, 10.0, 0.0, 10.0])

    assert mapOfBlocks.nbBlocks == 4

'''
test dictionnary on test.csv file
'''
def test_dictOfCoord_MapOfBlocks(spark_context):
    mapOfBlocks = MapOfBlocks.MapOfBlocks(80, 20)
    mapOfBlocks.setCoordinates([0.0, 10.0, 0.0, 10.0])
    mapOfBlocks.create_dict_coord()

    assert mapOfBlocks.dictOfCoord == { ((0.0,5.0),(0.0,5.0)) : 1 ,  ((0.0,5.0),(5.0,10.0)):2  , ((5.0,10.0),(0.0,5.0)):3  ,  ((5.0,10.0),(5.0,10.0)):4 }

'''
test the files creation WITHOUT MARGIN
'''
def test_partitioning_V1(spark_context):
    # get the name of the source path (file or directory)
    sourceDirectory = "./Source/test.csv"

    # initialize a MapOfBlocks object
    mapOfBlocks = MapOfBlocks.MapOfBlocks(80,20)

    # set min_ra, max_ra, min_decl, max_decl, step_ra, step_decl and create dict
    mapOfBlocks.setCoordinates(TP2_spark.getCoordinatesMinMax(sourceDirectory, spark_context))

    # first version
    mapOfBlocks.create_dict_coord()

    # get the name of the new directory
    resultDirectory = "./version1"
    result=TP2_spark.partitioning_V1(sourceDirectory, resultDirectory, spark_context, mapOfBlocks)

def test_getNbLinePerPatition_V1(spark_context):
    # get the name of the source path (file or directory)
    sourceDirectory = "./Source/test.csv"

    # initialize a MapOfBlocks object
    mapOfBlocks = MapOfBlocks.MapOfBlocks(80, 20)

    # set min_ra, max_ra, min_decl, max_decl, step_ra, step_decl and create dict
    mapOfBlocks.setCoordinates(TP2_spark.getCoordinatesMinMax(sourceDirectory, spark_context))

    # first version
    mapOfBlocks.create_dict_coord()

    #gets the number of lines per document
    results=TP2_spark.getNbLinePerPatition_V1(sourceDirectory,spark_context,mapOfBlocks)

    assert results== [(2, 2), (4, 2), (3, 2), (1, 1)]

def test_partitioning_V2(spark_context):
    # get the name of the source path (file or directory)
    sourceDirectory = "./Source/test.csv"

    # initialize a MapOfBlocks object
    mapOfBlocks = MapOfBlocks.MapOfBlocks(80,20)

    # set min_ra, max_ra, min_decl, max_decl, step_ra, step_decl and create dict
    mapOfBlocks.setCoordinates(TP2_spark.getCoordinatesMinMax(sourceDirectory, spark_context))

    # first version
    mapOfBlocks.create_dict_coord()

    # get the name of the new directory
    resultDirectory = "./version2"
    result=TP2_spark.partitioning_V2(sourceDirectory, resultDirectory, spark_context, mapOfBlocks)
    print(result)

'''
test the files creation WITH MARGIN
'''
def test_getNbLinePerPatition_V2(spark_context):
    # get the name of the source path (file or directory)
    sourceDirectory = "./Source/test.csv"

    # initialize a MapOfBlocks object
    mapOfBlocks = MapOfBlocks.MapOfBlocks(80,20)

    # set min_ra, max_ra, min_decl, max_decl, step_ra, step_decl and create dict
    mapOfBlocks.setCoordinates(TP2_spark.getCoordinatesMinMax(sourceDirectory, spark_context))

    # first version
    mapOfBlocks.create_dict_coord()

    results=TP2_spark.getNbLinePerPatition_V2(sourceDirectory,spark_context,mapOfBlocks)
    assert results== [('1', 1),('4', 2), ('3', 2),('2', 3)  ]


def test_writePropertiesInFile(spark_context):
    # get the name of the source path (file or directory)
    sourceDirectory = "./Source/test.csv"

    # initialize a MapOfBlocks object
    mapOfBlocks = MapOfBlocks.MapOfBlocks(80, 20)

    # set min_ra, max_ra, min_decl, max_decl, step_ra, step_decl and create dict
    mapOfBlocks.setCoordinates(TP2_spark.getCoordinatesMinMax(sourceDirectory, spark_context))

    # first version
    mapOfBlocks.create_dict_coord()

    mapOfBlocks.writePropertiesInFile("./Source")

    nbLinesPerBlocks = TP2_spark.getNbLinePerPatition_V2(sourceDirectory,spark_context,mapOfBlocks)
    TP2_spark.writeNbLinesInPropertiesFile("./Source",nbLinesPerBlocks)